{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uM-GhRoLv72-"},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import os\n","from torchsummary import summary\n","from sklearn.linear_model import LinearRegression\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error as mse\n","from sklearn.metrics import mean_absolute_error as mae\n"]},{"cell_type":"markdown","source":["# Code used to process the data and create a training and testing dataset"],"metadata":{"id":"toX43bsLqZTb"}},{"cell_type":"markdown","source":["We don't need this code for running the model and needed to run it just once for creating the pickled data files. So, we have commented it out and included it here"],"metadata":{"id":"0qcvaJCuqhfu"}},{"cell_type":"code","source":["# d = {\"Jan\": \"01\", \"Feb\": \"02\", \"Mar\": \"03\", \"Apr\": \"04\", \"May\": \"05\", \"Jun\": \"06\", \"Jul\": \"07\", \"Aug\": \"08\", \"Sep\": \"09\", \"Oct\": \"10\", \"Nov\": \"11\", \"Dec\":\"12\"}\n","# def date_transform(date):\n","#     month, year = date.split(\" \")\n","#     month = d[month]\n","#     res = int(year + month)\n","#     return res\n","\n","# inflation_df = pd.read_excel(f\"{project_dir}/inflation.xlsx\", sheet_name=\"fcpi_m\", index_col=0)\n","# us_fcpi = inflation_df.loc[\"USA\"]                                                               # monthly food cpi Series\n","\n","# ppd_df = pd.read_csv(f\"{project_dir}/ppd.csv\")\n","# ppd_df[\"Month/Year\"]= ppd_df[\"Month/Year\"].apply(date_transform)\n","# ppd_df = ppd_df[ppd_df[\"Month/Year\"] < 202203]                                                  # since the inflation data doenst not have 202203 onwards\n","\n","# # combine fcpi into ppd_df\n","# ppd_df[\"fcpi\"] = ppd_df[\"Month/Year\"].apply(lambda x : us_fcpi[x])\n","# ppd_df[\"Container or Bulk\"] = np.where(ppd_df[\"Container or Bulk\"] == \"Bulk\", 1, 0)\n","# ppd_df[\"Refrigerated or Dry\"] = np.where(ppd_df[\"Refrigerated or Dry\"],1, 0)\n","# ppd_df['Metric Tons'] *= ppd_df['Export or Import'].apply(lambda x: -1 if x=='Import' else 1) #metric tons value +ve if export and -ve if import\n","# ppd_df.drop(columns=['Export or Import', 'Year', 'Month'], inplace=True)\n","# ppd_df.to_pickle(f\"{project_dir}/ppd.pkl\")\n","\n","\n","# debug=ppd_df.sample(frac=0.05,random_state=1)\n","# train=ppd_df.drop(debug.index)\n","# debug.to_pickle(f\"{project_dir}/debug.pkl\")\n","# train.to_pickle(f\"{project_dir}/train.pkl\")"],"metadata":{"id":"MpH0tjgmwYSg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reading Data"],"metadata":{"id":"7QEZc2CwqwHW"}},{"cell_type":"code","source":["def getfile(location_pair,**kwargs):                                                                        #tries to get local version and then defaults to google drive version\n","    gdrive=location_pair\n","    loc = 'https://drive.google.com/uc?export=download&id='+gdrive.split('/')[-2]\n","    out=pd.read_pickle(loc,**kwargs)\n","    return out\n","\n","                                                                                                            # Read the data from the dataset file\n","fname=(\"https://drive.google.com/file/d/1PDrNKb-vcb6YH1o18EvvYEXf2a82_DJq/view?usp=share_link\")\n","ppd_df=getfile(fname)"],"metadata":{"id":"UDL2F-TYWusu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creating a mapping from port name to port ID"],"metadata":{"id":"xrVdZj3RrMDm"}},{"cell_type":"code","source":["port2num, num2port = dict(), dict()\n","num = 0\n","for port in ppd_df[\"Port\"]:\n","  if port in port2num: continue\n","  port2num[port] = num\n","  num2port[num] = port\n","  num += 1"],"metadata":{"id":"ixZhg95sxcjd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Our baseline Linear Regression Model:\n","It only takes care of the numerical features, and the MSE we got is 1143.94 for the Metric Tons data (for the readability of the loss, we divide the label by 10000), which will be the baseline performance of the model. We will add categorial data with one-hot encoding later on in our Deep learning model."],"metadata":{"id":"gQssMeygn5Wr"}},{"cell_type":"code","source":["dev = getfile(fname)\n","dev[\"Port\"] = dev[\"Port\"].apply(lambda x : port2num[x])\n","features = [\"TEU\", \"fcpi\"]                                                                              #Extracting relevant features\n","X = dev[features]\n","y = dev[\"Metric Tons\"] / 10000\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)               #Splitting dataset into 20% for test set and 80% for train set\n","losses = []\n","\n","model = LinearRegression()\n","model.fit(X_train, y_train)\n","pred = model.predict(X_test)\n","loss = mse(pred, y_test)                                                                                #Calculating loss\n","print(f\"Average MSE is {loss}\")"],"metadata":{"id":"rbH1ctwgy1Iv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670637535331,"user_tz":360,"elapsed":2198,"user":{"displayName":"Sahil Agrawal","userId":"17327048973671282633"}},"outputId":"e5f6230a-c008-4519-c999-5e64bf166f3d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average MSE is 1143.944210921282\n"]}]},{"cell_type":"markdown","source":["# Deep Learning Model\n","We have used an embedding layer for each categorical feature and then a linear layer to learn the important features. We finally combine the categorical and numerical features and run them through a dense network consisting of linear layers, a non-linearity and batch normalization layers."],"metadata":{"id":"vrlkKFMErses"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch\n","import torch.optim as optim\n","\n","y = dev[\"Metric Tons\"] / 10000\n","X = dev.loc[:, ~dev.columns.isin(['Metric Tons'])]\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)                      #Splitting into 20% test set and 80% train set\n","\n","class PortData(Dataset):                                                                                       #Class for the datasets\n","  def __init__(self, X, y):\n","    self.X1 = X[[\"TEU\", \"fcpi\"]].values\n","    self.X2 = X[[\"Port\", \"Container or Bulk\", \"Refrigerated or Dry\"]].values\n","    self.labels = y\n","    \n","  def __len__(self):\n","    return len(self.labels)\n","  \n","  def __getitem__(self, idx):\n","    return self.X1[idx], self.X2[idx], self.labels.iloc[idx]\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","train_dataset = PortData(X_train, y_train)\n","test_dataset = PortData(X_test, y_test)\n"],"metadata":{"id":"Ky0Q22HnWx5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ports_cnt = len(port2num)\n","class Network(nn.Module):\n","  def __init__(self):\n","    super(Network, self).__init__()\n","    self.port_embed = nn.Embedding(ports_cnt, 64)\n","    self.port_dense = nn.Linear(64, 64)\n","    self.bulk_embed = nn.Embedding(2, 3)\n","    self.bulk_dense = nn.Linear(3, 32)\n","    self.refri_embed = nn.Embedding(2, 3)\n","    self.refri_dense = nn.Linear(3, 32)\n","\n","\n","    self.full_dense = nn.Sequential(                                                               #Dense layer (mini batches greater than 4) after combining categorical and numerical features\n","        nn.Linear(130, 256),\n","        nn.BatchNorm1d(256),\n","        nn.LeakyReLU(),\n","        nn.Linear(256, 128),\n","        nn.LeakyReLU(),\n","        nn.Linear(128, 16),\n","        nn.LeakyReLU(),\n","        nn.Linear(16, 1)\n","    )\n","\n","    self.full_densebs1 = nn.Sequential(                                                           #Dense layer (mini batches lesser than or equal to 4) after combining categorical and numerical features\n","        nn.Linear(130, 256),\n","        nn.LeakyReLU(),\n","        nn.Linear(256, 128),\n","        nn.LeakyReLU(),\n","        nn.Linear(128, 16),\n","        nn.LeakyReLU(),\n","        nn.Linear(16, 1)\n","    )\n","  \n","  def forward(self, X_num, X_cat):                                                                 #Forward pass\n","    port_data = F.leaky_relu(self.port_embed(X_cat[:,0]))\n","    port_data = F.leaky_relu(self.port_dense(port_data))\n","    bulk_data= F.leaky_relu(self.bulk_embed(X_cat[:, 1]))\n","    bulk_data = F.leaky_relu(self.bulk_dense(bulk_data))\n","    refri_data = F.leaky_relu(self.refri_embed(X_cat[:,2]))\n","    refri_data = F.leaky_relu(self.refri_dense(refri_data))\n","\n","    all_combined = torch.cat((port_data, bulk_data, refri_data, X_num), 1)\n","    return self.full_dense(all_combined)\n","\n","\n","\n","def getOptim(m, lr=0.001, momen=0.1, op=\"Adam\"):                                                    #Chossing an optimizer\n","  if op == \"Adam\":\n","    return optim.Adam(m.parameters(), lr = lr)\n","  elif op == \"SGD\":\n","    return optim.SGD(m.parameters(), lr = lr, momentum = momen)\n","  elif op == \"RMS\":\n","    return optim.RMSprop(m.parameters(), lr = lr, momentum = momen)\n","\n","\n","def get_loader(batch_size = 32, stage = \"train\"):                                                   #Loading the data (pass the appropriate batch size into this function for mini batch GD)\n","  if (stage == \"train\"):\n","    return DataLoader(train_dataset, batch_size = batch_size)\n","  return DataLoader(test_dataset, batch_size = batch_size)\n","\n","\n","def train_minibatch(m, epoch=100, op=\"Adam\", lr=0.001, momen=0.1, batch_size=32):                   #Trainer used to compare different batch sizes for mini batch GD based on train and test error (Not actually\n","                                                                                                    #used for a single run of the model)\n","  for bs in [31955,512,256,128,64,32,16,8,4]:\n","      loss = nn.MSELoss()\n","      optim = getOptim(m,lr=lr, momen=momen, op=op)\n","      train_loader = get_loader(batch_size=bs)\n","      for e in range(epoch):\n","        curLoss = 0\n","        for i, data in enumerate(train_loader):\n","          X_num = data[0].float().to(device)\n","          X_cat = data[1].to(device)\n","          label = data[2].float().to(device)\n","          optim.zero_grad()\n","          if bs != 1:\n","            outputs = m(X_num, X_cat).squeeze()\n","          else:\n","            outputs = m(X_num, X_cat)\n","          l = loss(outputs, label)\n","          l.backward()\n","          optim.step()\n","          curLoss += l.item()\n","        if e == 99:\n","          print(f'{bs} batch size average Loss is: {curLoss / len(train_loader)}')\n","      \n","      \n","      test_loader = get_loader(bs, \"test\")\n","      all_loss=0\n","\n","      with torch.no_grad():\n","        loss = nn.MSELoss()\n","        for data in test_loader:\n","          X_num = data[0].float().to(device)\n","          X_cat = data[1].to(device)\n","          label = data[2].float().to(device)\n","          outputs = model(X_num, X_cat).squeeze()\n","          all_loss += (loss(outputs, label) * data[0].size(0))\n","\n","      mse = all_loss / len(test_dataset)\n","      print(f\"the mse of testing set is {mse}\")\n","\n","\n","def train(m, epoch=100, op=\"Adam\", lr=0.001, momen=0.1, batch_size=32):                             #Trainer actually used for a single run with the best parameters\n","  loss = nn.MSELoss()\n","  optim = getOptim(m,lr=lr, momen=momen, op=op)\n","  train_loader = get_loader(batch_size=batch_size)\n","  for e in range(epoch):\n","    curLoss = 0\n","    for i, data in enumerate(train_loader):\n","      X_num = data[0].float().to(device)                                                            #Separates the categorical and numerical features for separate handling\n","      X_cat = data[1].to(device)\n","      label = data[2].float().to(device)\n","      optim.zero_grad()\n","      outputs = m(X_num, X_cat).squeeze()\n","      l = loss(outputs, label)\n","      l.backward()\n","      optim.step()\n","      curLoss += l.item()\n","    if e % 10 == 0:\n","      print(f'{e}th epoch average Loss is: {curLoss / len(train_loader)}')                          #Printing the loss in the model\n","\n","model = Network().to(device)\n","train(model,batch_size = 128)"],"metadata":{"id":"Bkkjri2nbjze","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f003e47-4f24-4d84-a145-b8c56769377a","executionInfo":{"status":"ok","timestamp":1670637692703,"user_tz":360,"elapsed":143983,"user":{"displayName":"Sahil Agrawal","userId":"17327048973671282633"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0th epoch average Loss is: 1279.5553597564697\n","10th epoch average Loss is: 1062.7306218719482\n","20th epoch average Loss is: 1057.183671356201\n","30th epoch average Loss is: 1048.4794489746093\n","40th epoch average Loss is: 1046.6051750106813\n","50th epoch average Loss is: 1042.7283987960816\n","60th epoch average Loss is: 1040.1866698608399\n","70th epoch average Loss is: 1032.7182002792358\n","80th epoch average Loss is: 1025.8584277877808\n","90th epoch average Loss is: 1016.3845274734497\n"]}]},{"cell_type":"markdown","source":["Using SGD as the optimiser gave worse results (despite tuning learning rate and momentum) for the same architecture (as compared to Adam with all other parameters and hyperparameters constant). The 100th epoch average loss was in the range of 1100-1200 as compared to 600-700 for Adam. For RMS, it was found to be in the range 700-800."],"metadata":{"id":"VgO9F9DaEbyV"}},{"cell_type":"markdown","source":["Also, a batch size of 128 gave the best results for test set error (around 800) compared to other batch sizes or using the entire dataset (batch size = 31955) which gave test set errors of 900 to 1600"],"metadata":{"id":"mwAG2HXTu3KP"}},{"cell_type":"markdown","source":["# Testing the model on the Test Set\n","Running the model we trained above on the test set"],"metadata":{"id":"txgf_380sHSi"}},{"cell_type":"code","source":["batchSize = 128\n","test_loader = get_loader(batchSize, \"test\")\n","all_loss=0\n","\n","with torch.no_grad():\n","  loss = nn.MSELoss()\n","  for data in test_loader:\n","    X_num = data[0].float().to(device)\n","    X_cat = data[1].to(device)\n","    label = data[2].float().to(device)\n","    outputs = model(X_num, X_cat).squeeze()\n","    all_loss += (loss(outputs, label) * data[0].size(0))\n","\n","mse = all_loss / len(test_dataset)\n","print(f\"the mse of testing set is {mse}\")"],"metadata":{"id":"6ZeRUu-CmuzH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670637768378,"user_tz":360,"elapsed":252,"user":{"displayName":"Sahil Agrawal","userId":"17327048973671282633"}},"outputId":"e1dc86bf-a015-498e-cfa2-874b633685d5"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["the mse of testing set is 890.8041381835938\n"]}]},{"cell_type":"markdown","source":["# Saving the model\n","Note: Before running this block, please change project_dir to a location in your google drive where you want to store the trained model"],"metadata":{"id":"64b1PJ4BsMzm"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","\n","project_dir = \"/content/gdrive/MyDrive/CS547\"\n","torch.save(model.state_dict, f'{project_dir}/net.pth')"],"metadata":{"id":"GXhSjh15pC0h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"36273Ik6y_CV"},"execution_count":null,"outputs":[]}]}